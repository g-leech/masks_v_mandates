{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123456)\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from epimodel import EpidemiologicalParameters\n",
    "from epimodel.pymc3_models.mask_models import RandomWalkMobilityModel, MandateMobilityModel\n",
    "from epimodel.preprocessing.preprocess_mask_data import Preprocess_masks\n",
    "import epimodel.viz.region_plot as rp\n",
    "import epimodel.viz.prior_posterior as pp\n",
    "import epimodel.viz.yougov as yg\n",
    "import epimodel.viz.pred_cases as pc\n",
    "import epimodel.viz.result_plot as rep\n",
    "import epimodel.viz.mandate_wearing as mw\n",
    "import epimodel.viz.empirical_wearing as ew\n",
    "\n",
    "import calendar \n",
    "import theano.tensor as T\n",
    "import theano.tensor.signal.conv as C\n",
    "import theano\n",
    "\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import copy\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import json\n",
    "\n",
    "sns.set(style=\"ticks\", font='DejaVu Serif')\n",
    "PNAS_WIDTH_INCHES = 3.4252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"cases\" #args.model\n",
    "MASKS = \"wearing\" #args.masks\n",
    "W_PAR = \"exp\" #args.w_par\n",
    "\n",
    "n_mandates = 2\n",
    "\n",
    "MOBI = 'include' # args.mob\n",
    "US = True\n",
    "SMOOTH = False\n",
    "GATHERINGS = 3 #args.gatherings if args.gatherings else 3\n",
    "# MASKING = True # Always true\n",
    "\n",
    "if MODEL == \"both\":\n",
    "    TUNING = 2500\n",
    "else:\n",
    "    TUNING = 2000\n",
    "\n",
    "Ds = pd.date_range(\"2020-05-01\", \"2020-09-21\", freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data object\n",
    "\n",
    "path = f\"../data/modelling_set/master_data_mob_{MOBI}_us_{US}_m_w.csv\"\n",
    "masks_object = Preprocess_masks(path=path)\n",
    "\n",
    "masks_object.featurize(gatherings=GATHERINGS, masks=MASKS, smooth=SMOOTH, mobility=MOBI, n_mandates = 2)\n",
    "masks_object.make_preprocessed_object()\n",
    "data = masks_object.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# just mobility\n",
    "path = \"../data/modelling_set/master_data_mob_include_us_True_m_w.csv\"\n",
    "mobility_data = pd.read_csv(path)\n",
    "mobility_data = mobility_data.set_index([\"country\", \"date\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oxcgrt(use_us=True):\n",
    "    OXCGRT_PATH = \"../data/raw/OxCGRT_latest.csv\"\n",
    "    oxcgrt = pd.read_csv(OXCGRT_PATH, parse_dates=[\"Date\"], low_memory=False)\n",
    "    # Drop regional data\n",
    "    nat = oxcgrt[oxcgrt.Jurisdiction == \"NAT_TOTAL\"]\n",
    "\n",
    "    # Add US states\n",
    "    if use_us:\n",
    "        states = oxcgrt[\n",
    "            (oxcgrt.CountryName == \"United States\")\n",
    "            & (oxcgrt.Jurisdiction == \"STATE_TOTAL\")\n",
    "        ]\n",
    "        # Drop GEO to prevent name collision\n",
    "        nat = nat[nat.CountryName != \"Georgia\"]\n",
    "        states.CountryName = states.RegionName\n",
    "        states = states.replace(\"Georgia\", \"Georgia-US\")\n",
    "        nat = pd.concat([nat, states])\n",
    "\n",
    "    i = list(nat.columns).index(\"Date\")\n",
    "    nat.columns = list(nat.columns[:i]) + [\"date\"] + list(nat.columns[i + 1 :])\n",
    "\n",
    "    return nat[nat.date.isin(Ds)]\n",
    "\n",
    "\n",
    "maxes = {\n",
    "    \"C1\": 3, \n",
    "    \"C2\": 3, \n",
    "    \"C3\": 2, \n",
    "    \"C4\": 4, \n",
    "    \"C5\": 2, \n",
    "    \"C6\": 3, \n",
    "    \"C7\": 2, \n",
    "    \"C8\": 4, \n",
    "    \"H1\": 2\n",
    "}\n",
    "\n",
    "def subindex(df, c) :\n",
    "    x = df[c] - 0.5 * (1 - df[c[:2] + \"_Flag\"] )\n",
    "    return (x / maxes[c[:2]]) * 100\n",
    "\n",
    "\n",
    "# now with no weight on regional\n",
    "def nat_subindex(df, c) :\n",
    "    x = df[c] * df[c[:2] + \"_Flag\"] \n",
    "    return (x / maxes[c[:2]]) * 100\n",
    "\n",
    "# https://github.com/OxCGRT/covid-policy-tracker/blob/master/documentation/index_methodology.md\n",
    "def stringency(data, national=True):\n",
    "    if national: \n",
    "        sub = nat_subindex\n",
    "    else :\n",
    "        sub = subindex\n",
    "    \n",
    "    df = data.copy()\n",
    "    new_stringency_codes = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"H1\"]\n",
    "    nonflags = [c for c in data.columns if \"Flag\" not in c]\n",
    "    new_stringency_cols = [c for c in nonflags if c[:2] in new_stringency_codes]\n",
    "    \n",
    "    for c in new_stringency_cols:\n",
    "        if c[:2] != \"C8\":\n",
    "            df[c] = sub(df, c)\n",
    "        else :\n",
    "            df[c] = df[c] / maxes[c[:2]] * 100\n",
    "    \n",
    "    return df[new_stringency_cols].mean(axis=1).round(2)\n",
    "\n",
    "\n",
    "oxcgrt = load_oxcgrt()\n",
    "oxcgrt = oxcgrt[oxcgrt.date.isin(Ds)]\n",
    "oxcgrt[\"StringencyIndexNational\"] = stringency(oxcgrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rt.live/\n",
    "\n",
    "def load_rts() :\n",
    "    third_party_rts_us = pd.read_csv(\"../data/raw/us_rt_live.csv\", parse_dates=[\"date\"])\n",
    "    third_party_rts_us\n",
    "    us_state_abbrev = {    'Alabama': 'AL',    'Alaska': 'AK',    'American Samoa': 'AS',    'Arizona': 'AZ',    'Arkansas': 'AR',    'California': 'CA',    'Colorado': 'CO',    'Connecticut': 'CT',    'Delaware': 'DE',    'District of Columbia': 'DC',    'Florida': 'FL',    'Georgia': 'GA',    'Guam': 'GU',    'Hawaii': 'HI',    'Idaho': 'ID',    'Illinois': 'IL',    'Indiana': 'IN',    'Iowa': 'IA',    'Kansas': 'KS',    'Kentucky': 'KY',    'Louisiana': 'LA',    'Maine': 'ME',    'Maryland': 'MD',    'Massachusetts': 'MA',    'Michigan': 'MI',    'Minnesota': 'MN',    'Mississippi': 'MS',    'Missouri': 'MO',    'Montana': 'MT',    'Nebraska': 'NE',    'Nevada': 'NV',    'New Hampshire': 'NH',    'New Jersey': 'NJ',    'New Mexico': 'NM',    'New York': 'NY',    'North Carolina': 'NC',    'North Dakota': 'ND',    'Northern Mariana Islands':'MP',    'Ohio': 'OH',    'Oklahoma': 'OK',    'Oregon': 'OR',    'Pennsylvania': 'PA',    'Puerto Rico': 'PR',    'Rhode Island': 'RI',    'South Carolina': 'SC',    'South Dakota': 'SD',    'Tennessee': 'TN',    'Texas': 'TX',    'Utah': 'UT',    'Vermont': 'VT',    'Virgin Islands': 'VI',    'Virginia': 'VA',    'Washington': 'WA',    'West Virginia': 'WV',    'Wisconsin': 'WI',    'Wyoming': 'WY'}\n",
    "    us_state_abbrev = {value:key for key, value in us_state_abbrev.items()}\n",
    "    third_party_rts_us.region = third_party_rts_us.region.replace(us_state_abbrev)\n",
    "    third_party_rts_us.region = third_party_rts_us.region.replace({\"Georgia\": \"Georgia-US\"})\n",
    "    cols = ['date', 'region', 'mean', 'lower_80', 'upper_80']\n",
    "    third_party_rts_us = third_party_rts_us[cols]\n",
    "\n",
    "    # http://epidemicforecasting.org/country-rt-estimates\n",
    "    epifor_rts = pd.read_csv(\"../data/raw/r_estimates_epifor.csv\", parse_dates=[\"Date\"])\n",
    "    epifor_rts = epifor_rts[epifor_rts.EnoughData == 1]\n",
    "    epifor_rts = epifor_rts[epifor_rts.Date.isin(Ds)]\n",
    "    epifor_rts.Date = epifor_rts.Date.dt.date #pd.to_datetime(epifor_rts.Date, utc=True)\n",
    "\n",
    "    def recode(epifor_rts) :\n",
    "        with open(\"../data/raw/3166.json\") as f:\n",
    "            codes = json.load(f)\n",
    "            codes = codes['3166-1']\n",
    "\n",
    "        a2 = [c.get(\"alpha_2\") for c in codes]\n",
    "        names = [c.get(\"name\") for c in codes]\n",
    "        map_ = {c: n for c, n in zip(a2, names)}\n",
    "        epifor_rts[\"region\"] = epifor_rts.Code\n",
    "        epifor_rts.region = epifor_rts.Code.replace(map_)\n",
    "        epifor_rts.region = epifor_rts.region.replace({\"Tanzania, United Republic of\": \"Tanzania\"})\n",
    "        epifor_rts.region = epifor_rts.region.replace({\"Viet Nam\": \"Vietnam\"})\n",
    "        epifor_rts.region = epifor_rts.region.replace({\"Korea, Republic of\": \"South Korea\"})\n",
    "\n",
    "        return epifor_rts\n",
    "\n",
    "    epifor_rts = recode(epifor_rts)\n",
    "    epifor_rts.columns = [\"code\", \"date\", \"mean\", \"std\", \"enough\", \"region\"]\n",
    "\n",
    "    z80 = 0.842\n",
    "    epifor_rts[\"lower_80\"] = epifor_rts[\"mean\"] - z80 * epifor_rts[\"std\"]\n",
    "    epifor_rts[\"upper_80\"] = epifor_rts[\"mean\"] + z80 * epifor_rts[\"std\"]\n",
    "    #epifor_rts = epifor_rts[cols]\n",
    "    third_party_rts = pd.concat([epifor_rts, third_party_rts_us])\n",
    "    \n",
    "    # Correct for absence of case delay in Epifor\n",
    "    third_party_rts[\"mean_minus10\"] = third_party_rts[\"mean\"].shift(-10)\n",
    "    third_party_rts.date = pd.to_datetime(third_party_rts.date)\n",
    "    \n",
    "    return third_party_rts\n",
    "\n",
    "\n",
    "third_party_rts = load_rts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the empirical Rt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = third_party_rts[third_party_rts.date.isin(Ds)]\n",
    "relevant = relevant[relevant.region.isin(data.Rs)]\n",
    "relevant[\"mean_minus10\"].mean(), relevant[\"mean_minus10\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How many regions?:\", len(data.Rs))\n",
    "print(\"How many NPIs?:\", len(data.CMs))\n",
    "print(\"How many days?:\", len(data.Ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandate_pkl_5pct = \"mandate_2and3_cases_countries_92_05-28-16:44.pkl\"\n",
    "mandate_pkl_0 = \"mandate_2and3_cases_countries_92_06-04-16:37.pkl\"\n",
    "q2_pkl_1000_250 = \"zeroed_wearing_hyper_Rs_wearing_log_quadratic_2_cases_countries_92_05-25-18:37.pkl\"\n",
    "exp_pkl = \"wearing_exp_cases_countries_92_05-31-02:56.pkl\" # 1000 + 700 \n",
    "ll_pkl = \"wearing_log_linear_cases_countries_92_06-05-16:46.pkl\"\n",
    "\n",
    "\n",
    "def load_pickle(p) :\n",
    "    path = 'pickles/' + p\n",
    "    with open(path, 'rb') as buff:\n",
    "        trace = pickle.load(buff)\n",
    "\n",
    "    colfile = path[:-4] + \"_cols\"\n",
    "    \n",
    "    cf = Path(colfile)\n",
    "    if cf.is_file():\n",
    "        with open(colfile, \"r\") as f:\n",
    "            npi_cols = f.read().split(\", \")\n",
    "    else :\n",
    "        print(\"cols missing\")\n",
    "        npi_cols = []\n",
    "    \n",
    "    return trace, npi_cols\n",
    "\n",
    "\n",
    "m_trace, _ = load_pickle(mandate_pkl_0)\n",
    "exp_trace, npi_cols = load_pickle(exp_pkl)\n",
    "#q2_trace, npi_cols = load_pickle(q2_pkl_1000_250)\n",
    "#ll_trace, _ = load_pickle(ll_pkl)\n",
    "\n",
    "print(\"Pickle has\", exp_trace.RegionR.shape[1], \"regions\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "varnames = [c for c in exp_trace.varnames if c not in [\"Psi_log__\", \"Infected_log\", \"HyperRVar_log__\", \"GrowthNoiseScale_log__\", \"r_walk_noise_scale_log__\"]]\n",
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = exp_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ns = npi_cols.copy()\n",
    "\n",
    "v = [\"CMReduction\"]\n",
    "if \"WearingReduction\" in m_trace.varnames :\n",
    "    v += [\"WearingReduction\"]\n",
    "if \"MandateReduction\" in m_trace.varnames :\n",
    "    v += [\"MandateReduction\"]\n",
    "if \"MobilityReduction\" in m_trace.varnames :\n",
    "    v += [\"MobilityReduction\"]\n",
    "    #ns.remove(\"avg_mobility_no_parks_no_residential\")\n",
    "    #ns += [\"MobilityReduction\"]\n",
    "\n",
    "\n",
    "s = pm.summary(m_trace, var_names=v, hdi_prob=0.95)\n",
    "#s.index = ns\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance as AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(samples) :\n",
    "    return 1 - np.sum(samples < 1) / len(samples)\n",
    "\n",
    "p(exp_trace.WearingReduction), p(m_trace.MandateReduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_renamed() :\n",
    "    plots = pm.plots.plot_posterior(trace, var_names=[\"WearingReduction\", \"MobilityReduction\"], hdi_prob=0.95)\n",
    "    n = npi_cols\n",
    "#     for i, p in enumerate(plots):\n",
    "#         p.set_title(n[i])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# plot_posterior_renamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "ess = az.ess(exp_trace)\n",
    "rhat = az.rhat(exp_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_minus_logs = [v for v in varnames if v not in [\"r_walk_noise_scale_log__\", \"HyperRMean_lowerbound__\"]]\n",
    "\n",
    "\n",
    "def collate(stat, varnames):\n",
    "    stat_all = []\n",
    "    stat_nums = []\n",
    "    for var in varnames :\n",
    "        if stat[str(var)].size > 1:\n",
    "            stat_all.append(stat[str(var)].to_dataframe().to_numpy().flatten())\n",
    "        else:\n",
    "            stat_nums.append(float(stat[str(var)]))\n",
    "    stat_all = np.concatenate(np.array(stat_all))\n",
    "    stat_all = np.concatenate([stat_all, stat_nums])\n",
    "    # stat_all[stat_all > 100] = 1\n",
    "    return stat_all\n",
    "\n",
    "def diagnostics(tr) :\n",
    "    for r in tr.varnames :\n",
    "        rhat = pm.rhat(tr)[r]\n",
    "        print(f'Rhat({r}) = {rhat}')\n",
    "\n",
    "cols = sns.cubehelix_palette(3, start=0.2, light=0.6, dark=0.1, rot=0.2)\n",
    "\n",
    "PNAS_WIDTH_INCHES = 3.4252\n",
    "plt.figure(figsize=(PNAS_WIDTH_INCHES * 1.5, 2), dpi=400)\n",
    "plt.subplot(121)\n",
    "plt.hist(collate(rhat, vs_minus_logs), bins=40, color=cols[0])\n",
    "plt.title(\"$\\hat{R}$\", fontsize=8)\n",
    "ylabels = ['{:,.0f}'.format(x) + 'k' for x in plt.gca().axes.get_yticks()/1000]\n",
    "plt.gca().set_yticklabels(ylabels)\n",
    "plt.ylabel(\"Number of parameters\", fontsize=8)\n",
    "\n",
    "def get_total_samples(t) :\n",
    "    l = str(t).replace(\"<MultiTrace: \", \"\").split(\",\")\n",
    "    return int(l[0].split()[0]) * int(l[1].split()[0])\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(collate(ess, vs_minus_logs), bins=40, color=cols[0]) # / samples\n",
    "#plt.xlim() # [0,2] \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.title(\"ESS\", fontsize=8) \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"../outputs/mcmc_wearing.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior / posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(pp)\n",
    "pp.plot_all_pps(exp_trace, m_trace)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../outputs/pp_grid.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits and holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp.reload(pc)\n",
    "\n",
    "# for region in data.Rs :\n",
    "#     pc.epicurve_plot(data, oxcgrt, trace, region)\n",
    "\n",
    "\n",
    "    \n",
    "pred_rs = [\"Australia\", \"United Kingdom\", \"Nigeria\", \"Singapore\"] \n",
    "f, (rowax, colax) = plt.subplots(2, 2, figsize=(10,6), dpi=500, sharex=True)\n",
    "pc.epicurve_plot(data, oxcgrt, Ds, exp_trace, pred_rs[0], rowax[0], leg=True)\n",
    "pc.epicurve_plot(data, oxcgrt,  Ds, exp_trace, pred_rs[1], rowax[1])\n",
    "pc.epicurve_plot(data, oxcgrt,  Ds, exp_trace, pred_rs[2], colax[0])\n",
    "pc.epicurve_plot(data, oxcgrt,  Ds, exp_trace, pred_rs[3], colax[1])\n",
    "\n",
    "\n",
    "plt.savefig(f\"../outputs/pred_curves_4.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(rp)\n",
    "\n",
    "pred_rs = [\"India\", \"Sweden\", \"United Kingdom\", \"South Africa\"]\n",
    "for r in pred_rs: #data.Rs :\n",
    "    rp.reprod_plot(exp_trace, data, mobility_data, oxcgrt, third_party_rts, r, start_d_i=0) \n",
    "    #plt.show()\n",
    "    plt.savefig(f\"../outputs/region_plots_{r}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mob vs Wearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "df = masks_object.df\n",
    "\n",
    "# corrs = []\n",
    "# for c in mobility_data.reset_index().country.unique() :\n",
    "#     if c not in df.reset_index().country.unique() :\n",
    "#         continue\n",
    "    \n",
    "#     mobc = mobility_data.loc[c]\n",
    "#     cdf = df.loc[c]\n",
    "#     j = cdf[[\"percent_mc\"]].join(mobc.avg_mobility_no_parks_no_residential)\n",
    "#     corr = np.corrcoef(j.avg_mobility_no_parks_no_residential, j.percent_mc)\n",
    "#     if corr[0][1] != np.nan :\n",
    "#         corrs.append(corr[0][1])\n",
    "\n",
    "# corrs = [c for c in corrs if not np.isnan(c)]\n",
    "#np.mean(corrs)\n",
    "\n",
    "# print(\"rho (overall)\", pearsonr(df.avg_mobility_no_parks_no_residential, df[\"percent_mc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandate vs wearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "df = mobility_data\n",
    "\n",
    "print(\"rho (binary overall)\", pearsonr(df.percent_mc, df[\"H6_Facial Coverings\"])[0])\n",
    "\n",
    "## Simpson's paradox\n",
    "# corrs = []\n",
    "# for c in df.reset_index().country.unique() :\n",
    "#     cdf = df.loc[c]\n",
    "#     corr = pearsonr(cdf.percent_mc, cdf[\"H6_Facial Coverings\"])[0]\n",
    "#     if corr != np.nan :\n",
    "#         corrs.append(corr)\n",
    "# corrs = [c for c in corrs if not np.isnan(c)]\n",
    "# print(\"rho (binary by country)\", np.mean(corrs))#, np.std(corrs))\n",
    "\n",
    "# or original 4-level H6:\n",
    "df2 = df.reset_index()\n",
    "df2[\"date\"] = pd.to_datetime(df2[\"date\"])\n",
    "df2 = df2[[\"country\", \"date\", \"percent_mc\"]]\n",
    "df2 = df2.set_index([\"country\", \"date\"])\n",
    "ourox = oxcgrt[oxcgrt.CountryName.isin(data.Rs)][[\"CountryName\", \"date\", \"H6_Facial Coverings\"]]\n",
    "ourox.columns = [\"country\", \"date\", \"H6_Facial Coverings\"]\n",
    "ourox = ourox.set_index([\"country\", \"date\"])\n",
    "ourox = ourox.join(df2)\n",
    "\n",
    "print(\"Original 4-level rho\", pearsonr(ourox.percent_mc, ourox[\"H6_Facial Coverings\"])[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(mw)\n",
    "import epimodel.viz.mandate_wearing as mw\n",
    "\n",
    "\n",
    "def post_mandate_rise(Rs, df):\n",
    "    ranges = []\n",
    "    d = mw.get_centred_summary(Rs, df)\n",
    "    for c in d.country.unique() :\n",
    "        last = d[(d.country == c) & (d.day > 23)]\n",
    "        first = d[(d.country == c) & (d.day < 7)]\n",
    "        if len(last) :\n",
    "            last = last.percent_mc.mean()\n",
    "            first = first.percent_mc.mean()\n",
    "            ranges.append( last - first )\n",
    "    \n",
    "    return ranges\n",
    "\n",
    "rises = post_mandate_rise(data.Rs, df)\n",
    "piles = np.percentile(rises, [2.5, 50, 97.5])\n",
    "\n",
    "\n",
    "def exp_reduction_vector(a, x) :\n",
    "    reductions = 1 - np.exp((-1.0) * a * x)\n",
    "    return reductions\n",
    "\n",
    "a = exp_trace.Wearing_Alpha.mean()\n",
    "exp_reduction_vector(a, piles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reset_index()\n",
    "# df[df.country == \"Bangladesh\"][[\"percent_mc\"]].plot()\n",
    "# plt.ylim(0,1)\n",
    "summaries = mw.get_centred_summary(data.Rs, df)\n",
    "len(summaries.country.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = mobility_data\n",
    "\n",
    "imp.reload(mw)\n",
    "\n",
    "\n",
    "#mw.mandate_barplot(df)\n",
    "#mw.mandate_distplot(df)\n",
    "#mw.messy_centred_mandate_plot(data.Rs, df)\n",
    "#mw.original_stringency_plot(ourox)\n",
    "#mw.centred_mandate_plot(data.Rs, df)\n",
    "\n",
    "mw.country_centred_mandate_plot(data.Rs, df)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(\"../outputs/mw_panels.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wearing_reduction_samples.txt\", \"r\") as f :\n",
    "    wred = f.read()\n",
    "    wred = np.array(wred.split(\"\\n\")[:-1]).astype(np.float64)\n",
    "\n",
    "with open(\"mandate_reduction_samples.txt\", \"r\") as f :\n",
    "    mred = f.read()\n",
    "    mred = np.array(mred.split(\"\\n\")[:-1]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import epimodel.viz.average_trends as at\n",
    "# import imp\n",
    "# imp.reload(at)\n",
    "# df = mobility_data\n",
    "\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(15,7), dpi=500)\n",
    "# gs = fig.add_gridspec(3,6)\n",
    "# ax00 = fig.add_subplot(gs[0, 0])\n",
    "# ax01 = fig.add_subplot(gs[0, 1])\n",
    "# ax10 = fig.add_subplot(gs[1, 0])\n",
    "# ax11 = fig.add_subplot(gs[1, 1])\n",
    "# ax3 = fig.add_subplot(gs[1, 2:])\n",
    "# ax4 = fig.add_subplot(gs[0, 2:4])\n",
    "# ax5 = fig.add_subplot(gs[0, 4:])\n",
    "\n",
    "# img = mpimg.imread('../outputs/val-1.png')\n",
    "# ax3.imshow(img, aspect='auto')\n",
    "# ax3.axis('off')\n",
    "# ax3.set_title(\"G\", loc=\"left\", fontweight=\"bold\")\n",
    "\n",
    "\n",
    "# Ds = pd.to_datetime(Ds)\n",
    "# at.plot_avg_daily_new(df, Ds, ax00)\n",
    "# at.plot_mandates(df, Ds, ax01)\n",
    "# at.plot_mob(df, Ds, ax11)\n",
    "# at.plot_wearing(df, Ds, ax10)\n",
    "\n",
    "\n",
    "# def main_result_posteriors(mred, wred, ax):\n",
    "#     sns.kdeplot(wred, label=\"wearing\", shade=True, ax=ax)    \n",
    "#     sns.kdeplot(mred, label=\"mandate\", color=\"green\", shade=True, ax=ax)\n",
    "#     ax.axvline(x=0, color=\"black\", linestyle=\"--\")\n",
    "#     ax.set_xlabel(\"Inferred % reduction in R\", fontsize=16)\n",
    "#     ax.set_xlim(-20, 60)\n",
    "#     ax.axes.get_yaxis().set_visible(False)\n",
    "#     ax.legend(fontsize=12, frameon=False)\n",
    "\n",
    "    \n",
    "# def exp_reduction(a, x):\n",
    "#     reductions = 1 - np.exp((-1.0) * a * x)\n",
    "#     return reductions.mean()\n",
    "\n",
    "\n",
    "# def get_median_reduction(a, df):\n",
    "#     obs_ = []\n",
    "#     r = exp_reduction\n",
    "\n",
    "#     for c in df.reset_index().country.unique():\n",
    "#         cdf = df.loc[c]\n",
    "#         median_ = cdf.percent_mc.median()\n",
    "#         med_reduction_r = r(a, median_)\n",
    "#         actual = med_reduction_r\n",
    "#         obs_.append(actual * 100)\n",
    "\n",
    "#     return obs_\n",
    "    \n",
    "# def plot_median_wearing_effect(df, alpha, ax):\n",
    "#     obs_ = get_median_reduction(alpha, df)\n",
    "#     print(np.percentile(obs_, [2.5, 50, 97.5]))\n",
    "#     sns.distplot(obs_, kde=True, hist=False, kde_kws={\"shade\": True}, ax=ax)\n",
    "\n",
    "#     ax.set_xlabel(\"% R reduction (by regional wearing level)\", fontsize=16)\n",
    "#     ax.yaxis.set_ticks([])\n",
    "#     ax.set_xlim(0, 30)\n",
    "\n",
    "#     med = np.median(obs_)\n",
    "#     print(med)\n",
    "#     ax.axvline(x=med, color=\"black\", linestyle=\"--\", label=\"median\")\n",
    "\n",
    "#     ax.legend(fontsize=12, frameon=False, loc=\"upper left\")\n",
    "\n",
    "\n",
    "# main_result_posteriors(mred, wred, ax=ax4)\n",
    "# ax4.set_title(\"E\", loc='left', fontweight=\"bold\")\n",
    "\n",
    "# df = mobility_data\n",
    "# alpha = exp_trace.Wearing_Alpha\n",
    "# plot_median_wearing_effect(df, alpha, ax=ax5)\n",
    "# ax5.set_title(\"F\", loc='left', fontweight=\"bold\")\n",
    "\n",
    "# plt.tight_layout(pad=1.2)\n",
    "# #plt.subplots_adjust(top=0.9)#, right=1)\n",
    "\n",
    "# plt.savefig(\"../outputs/fig3_placeholder.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 1: Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(rep)\n",
    "\n",
    "PNAS_WIDTH_INCHES = 3.4252\n",
    "fig, ax = plt.subplots(figsize=(PNAS_WIDTH_INCHES,PNAS_WIDTH_INCHES * 1.5), dpi=400)\n",
    "ax = plt.subplot(2,1,1)\n",
    "rep.main_result_posteriors(m=m_trace, w=exp_trace, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=1.3)\n",
    "plt.title(\"A\", loc='left', fontweight=\"bold\")\n",
    "\n",
    "df = mobility_data\n",
    "imp.reload(ew)\n",
    "ax = plt.subplot(2,1,2)\n",
    "plt.title(\"B\", loc='left', fontweight=\"bold\")\n",
    "ew.plot_median_wearing_effect(df, exp_trace, \"exp\", ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/main_results_vertical.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the empirical wearing effect in this window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(ew)\n",
    "\n",
    "df = masks_object.df\n",
    "\n",
    "#ew.plot_actual_wearing_effect(df, exp_trace, \"exp\")\n",
    "#ew.plot_max_wearing_effect(df, exp_trace, \"exp\", ax)\n",
    "\n",
    "maxes = {c : df.loc[c].percent_mc.max()  for c in df.reset_index().country.unique()}\n",
    "\n",
    "g = df.reset_index()[[\"country\", \"percent_mc\"]].groupby(\"country\")\n",
    "ranges = g.max(\"percent_mc\") - g.min(\"percent_mc\")\n",
    "print(\"Average change in wearing in window:\", round(ranges.median().iloc[0], 3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 3: R reduction over wearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exp_reduction_vector(a, x) :\n",
    "    reductions = 1 - np.exp((-1.0) * a * x)\n",
    "    return reductions\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def ll_reduction_vector(a, x) :\n",
    "    w = a * x \n",
    "    reductions = - np.log(relu(1 - w))\n",
    "    return reductions\n",
    "\n",
    "\n",
    "def q2_reduction_vector(alphas, x) :\n",
    "    w = alphas[0] * x + alphas[1] * x**2\n",
    "    reductions = - np.log(relu(1 - w))\n",
    "    return reductions\n",
    "\n",
    "\n",
    "def plot_param(tr, r, t, ax):\n",
    "    alpha = tr.Wearing_Alpha.mean()\n",
    "    \n",
    "    if tr.Wearing_Alpha.shape[1] > 1 :\n",
    "        lu0, m0, hi0 = np.percentile(tr.Wearing_Alpha[0], [2.5, 50, 97.5])\n",
    "        lu1, m1, hi1 = np.percentile(tr.Wearing_Alpha[1], [2.5, 50, 97.5])\n",
    "        alpha = [m0, m1]\n",
    "        lu = [lu0, lu1]\n",
    "        hi = [hi0, hi1]\n",
    "    else:\n",
    "        lu, alpha, hi = np.percentile(tr.Wearing_Alpha, [2.5, 50, 97.5])\n",
    "    \n",
    "    i = data.CMs.index(\"percent_mc\")\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    y = r(alpha, x)\n",
    "    \n",
    "    #plt.plot(x, y)\n",
    "    ax.plot(x * 100, y * 100)\n",
    "    \n",
    "    los = r(lu, x) \n",
    "    his = r(hi, x)\n",
    "    ax.fill_between(x * 100, los * 100, his * 100, alpha=0.2)\n",
    "    \n",
    "    ax.set_title(t, fontsize=10)\n",
    "    ax.set_ylim(0,60)\n",
    "    ax.set_xlim(0,100)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "    ax.xaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "\n",
    "f, (ax1,ax2,ax3) = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(5,7), dpi=400)\n",
    "plot_param(exp_trace, exp_reduction_vector, \"Exponential feature\", ax1)\n",
    "#plt.set_ylabel(\"\", fontsize=16)\n",
    "f.text(0.5, -0.02, '% mask wearing', ha='center', fontsize=16)\n",
    "f.text(-0.02, 0.5, \"Reduction in R\", va='center', fontsize=16, rotation='vertical')\n",
    "\n",
    "plot_param(q2_trace, q2_reduction_vector, \"Quadratic feature\", ax2)\n",
    "plot_param(ll_trace, ll_reduction_vector, \"Linear feature\", ax3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/wearing_paramets.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max mandate effect on wearing in first wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(yg)\n",
    "\n",
    "\n",
    "yg.plot_earliest_mandates_against_wearing()\n",
    "\n",
    "yg.get_before_after_mandate_change()\n",
    "\n",
    "\n",
    "plt.savefig(\"../outputs/yg_first_wave.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", font='DejaVu Serif', font_scale=0.3)\n",
    "\n",
    "\n",
    "def post_alpha_corrs(tr, w, ax):\n",
    "    if w == \"Wearing\":\n",
    "        m = \"WearingReduction\"\n",
    "    else :\n",
    "        m = \"MandateReduction\"\n",
    "    df_exp = pm.trace_to_dataframe(tr, varnames = ['CMReduction', m, 'MobilityReduction'])\n",
    "\n",
    "    df_exp.columns = npi_cols[:-2] + [m, \"mobility\"]\n",
    "    corrs = df_exp.corr()\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corrs, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(\n",
    "        corrs,\n",
    "        mask=mask,\n",
    "        #cmap=cmap,\n",
    "        #vmax=1,\n",
    "        linewidths=0.5,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        vmin=-1, \n",
    "        vmax=1,\n",
    "        center=0,\n",
    "        cbar=False,\n",
    "        ax=ax,\n",
    "    );\n",
    "    #ax.set_ylim(0, -1)\n",
    "    #ax.yaxis.get_major_ticks()[0].label.visible = False\n",
    "    ax.axes.get_xticklabels()[0].set_visible(False)\n",
    "    \n",
    "    \n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(3.5, 7), dpi=700)\n",
    "\n",
    "\n",
    "post_alpha_corrs(exp_trace, w=\"Wearing\", ax=ax1)\n",
    "ax1.set_title(f\"Wearing model, posterior correlations in reductions\", fontsize=6)\n",
    "\n",
    "post_alpha_corrs(m_trace, w=\"Mandate\", ax=ax2)\n",
    "ax2.set_title(f\"\\n\\nMandate model, posterior correlations in reductions\", fontsize=6)\n",
    "\n",
    "f.subplots_adjust(hspace=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(\"../outputs/posterior_corrs_mandate.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMD facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds = pd.date_range(\"2020-05-01\", \"2020-09-21\", freq=\"D\")\n",
    "wearing = pd.read_csv(\n",
    "        \"../data/raw/umd/umd_national_wearing.csv\",\n",
    "        parse_dates=[\"survey_date\"],\n",
    "        infer_datetime_format=True,\n",
    "    ).drop_duplicates()\n",
    "wearing_windowed = wearing[(wearing.survey_date >= Ds[0]) & (wearing.survey_date <= Ds[-1])]\n",
    "\n",
    "wearing_windowed.sample_size.sum()\n",
    "\n",
    "\n",
    "wearing2021 = pd.read_csv(\n",
    "        \"../data/raw/umd/umd_national_wearing_2021.csv\",\n",
    "        parse_dates=[\"survey_date\"],\n",
    "        infer_datetime_format=True,\n",
    "    ).drop_duplicates()\n",
    "\n",
    "\n",
    "wearing = pd.concat([wearing, wearing2021])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wearing.percent_mc.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 8\n",
    "sns.reset_orig()\n",
    "plt.rcParams['font.family'] = \"DejaVu Serif\"\n",
    "# y = wearing[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median()\n",
    "# plt.plot(y)\n",
    "# lo = wearing[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").quantile(0.025)\n",
    "# hi = wearing[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").quantile(0.975)\n",
    "# plt.plot(y, color=\"blue\")\n",
    "# #plt.fill_between(lo.index, lo.percent_mc, hi.percent_mc, alpha=0.1)\n",
    "# plt.title(\"World median wearing percentage\")\n",
    "# plt.ylabel(\"% wearing\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(PNAS_WIDTH_INCHES, 2.2), dpi=400)\n",
    "\n",
    "vax_countries = [\"Israel\", \"United Kingdom\", \"United States\", \"Canada\", \"Chile\", \"Hungary\", \"Germany\", \"Italy\", \"France\", \"Mongolia\", \"Uruguay\", \"Qatar\", \"Finland\", \"Belgium\", \"Italy\", \"Spain\", \"Netherlands\", \"Iceland\", \"Bahrain\", \"Bhutan\", \"Cyprus\", \"Malta\", \"United Arab Emirates\", \"Austria\"]\n",
    "vax = wearing[wearing.country.isin(vax_countries)]\n",
    "vax.percent_mc = vax.percent_mc.rolling(3).mean()\n",
    "\n",
    "y = vax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median()\n",
    "lu = vax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").quantile(0.25)\n",
    "hi = vax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").quantile(0.75)\n",
    "# plt.plot(y * 100, label=\"> 40% vaccinated\")\n",
    "# plt.plot(lu.percent_mc * 100, alpha=0.5, color=\"blue\")\n",
    "# plt.plot(hi.percent_mc * 100, alpha=0.5, color=\"blue\")\n",
    "#plt.fill_between(lu.index, lu.percent_mc * 100, hi.percent_mc * 100, alpha=0.1)\n",
    "\n",
    "notvax = wearing[~wearing.country.isin(vax_countries)]\n",
    "notvax.percent_mc = notvax.percent_mc.rolling(3).mean()\n",
    "\n",
    "#notvax.percent_mc = notvax.percent_mc.interpolate(limit_direction='backward', limit=2)\n",
    "y = notvax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median()\n",
    "y.percent_mc.iloc[0:3] = 0.654529\n",
    "lu = notvax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").quantile(0.25)\n",
    "lu.percent_mc.iloc[0:3] = 0.52964\n",
    "hi = notvax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").quantile(0.75)\n",
    "# plt.title(\"World median wearing\")\n",
    "plt.plot(y * 100, label=\"median\",  linewidth=0.5)#\"< 40% vaccinated\")\n",
    "plt.fill_between(lu.index, lu.percent_mc * 100, hi.percent_mc * 100, alpha=0.2, label=\"50% CI\")\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "plt.ylim(40, 100)\n",
    "#plt.xlim(y.index[0], y.index[-1])\n",
    "#plt.xlim()\n",
    "\n",
    "\n",
    "plt.ylabel(\"% wearing\", fontsize=10)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.xlabel(\"\", fontsize=10)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.legend(loc=\"lower right\", frameon=False, fontsize=8)\n",
    "\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "locator = mdates.MonthLocator()  # every month\n",
    "fmt = mdates.DateFormatter(\"%b'%y\")\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "\n",
    "# for label in ax.xaxis.get_ticklabels()[1:][::4]:\n",
    "#     label.set_visible(False)\n",
    "\n",
    "# for label in ax.xaxis.get_ticklabels()[1:][::2]:\n",
    "#     label.set_visible(False)\n",
    "    \n",
    "for i, label in enumerate(ax.xaxis.get_ticklabels()[1:]):\n",
    "    if i % 2 == 0:\n",
    "        label.set_visible(False)\n",
    "\n",
    "for i, label in enumerate(ax.xaxis.get_ticklabels()[1:]):\n",
    "    if i % 2 == 0:\n",
    "        label.set_visible(False)\n",
    "    \n",
    "#plt.rcParams['axes.linewidth'] = 0.001\n",
    "#[i.set_linewidth(0.1) for i in ax.spines()]\n",
    "    \n",
    "plt.savefig(\"../outputs/world_wearing_21.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vax = wearing[wearing.country.isin(vax_countries)]\n",
    "vax.percent_mc = vax.percent_mc.rolling(7).mean()\n",
    "\n",
    "\n",
    "notvax = wearing[~wearing.country.isin(vax_countries)]\n",
    "notvax.percent_mc = notvax.percent_mc.rolling(7).mean()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(PNAS_WIDTH_INCHES, 1.5), dpi=400)\n",
    "\n",
    "for c in vax.country.unique() :\n",
    "    y = vax[vax.country == c][[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median()\n",
    "    plt.plot(y * 100, alpha=0.5, color=\"blue\")\n",
    "\n",
    "y = vax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median()\n",
    "plt.plot(y * 100, color=\"blue\")\n",
    "    \n",
    "for c in notvax.country.unique() :\n",
    "    y = notvax[notvax.country == c][[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median()\n",
    "    plt.plot(y * 100, alpha=0.5, color=\"#CCC\")\n",
    "\n",
    "y = notvax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median()\n",
    "plt.plot(y * 100, color=\"grey\")\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig(\"../outputs/world_wearing_21.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout nonwearers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.iloc[-7:-1].mean()\n",
    "\n",
    "y = wearing[[\"percent_mc\", \"country\"]].groupby(\"country\").max()\n",
    "#plt.bar(y.index, y.percent_mc)\n",
    "y.sort_values(\"percent_mc\")\n",
    "\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How much did the nonvax drop in May 2021?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_notvax = notvax[[\"percent_mc\", \"survey_date\"]].groupby(\"survey_date\").median().tail(31)\n",
    "\n",
    "earlyMay = may_notvax.head(3).mean() \n",
    "lateMay = may_notvax.tail(2).mean() \n",
    "1 - (earlyMay / lateMay)\n",
    "\n",
    "#may_notvax.head(12).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many NPIs are active at the start? Which are \"reopening\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reset_index()\n",
    "start_df = df[df.date == \"2020-05-01\"]\n",
    "npis = [c for c in df.columns[4:15]]\n",
    "\n",
    "\n",
    "ps = []\n",
    "for npi in npis:\n",
    "    pct_1 = start_df[npi].value_counts(normalize=True).loc[1]\n",
    "    print(npi, f\"{pct_1:.1f}\")\n",
    "    ps.append(pct_1)\n",
    "    \n",
    "np.mean([0.9, 0.8, 0.6, 0.5, 0.8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(df, col, t):\n",
    "    NATIONAL = 1\n",
    "    code = col[:2]\n",
    "\n",
    "    return (df[col] >= t) & (df[f\"{code}_Flag\"] == NATIONAL)\n",
    "\n",
    "def load_and_clean_wearing():\n",
    "    wearing = pd.read_csv(\n",
    "        \"../data/raw/umd/umd_national_wearing.csv\",\n",
    "        parse_dates=[\"survey_date\"],\n",
    "        infer_datetime_format=True,\n",
    "    ).drop_duplicates()\n",
    "    wearing = wearing[(wearing.survey_date >= Ds[0]) & (wearing.survey_date <= Ds[-1])]\n",
    "    cols = [\"country\", \"survey_date\", \"percent_mc\"]\n",
    "    wearing = wearing[cols]\n",
    "    cols = [\"country\", \"date\", \"percent_mc\"]\n",
    "    wearing.columns = cols\n",
    "\n",
    "    # Append US\n",
    "    us_wearing = load_and_clean_rader()\n",
    "    us_wearing.columns = [\"date\", \"country\", \"percent_mc\"]\n",
    "    us_wearing = us_wearing[cols]\n",
    "    us_wearing = us_wearing.replace(\"Georgia\", \"Georgia-US\")\n",
    "    us_wearing = us_wearing.replace(\"District of Columbia (DC)\", \"District of Columbia\")\n",
    "    # Add dummy wearing back to 1st May\n",
    "    us_wearing = add_dummy_wearing_us(us_wearing, backfill=True)\n",
    "    wearing = pd.concat([wearing, us_wearing])\n",
    "\n",
    "    return fill_missing_days(wearing)\n",
    "\n",
    "def load_and_clean_rader(THRESHOLD=2, SMOOTH_RADER=True):  # or less\n",
    "    DATA_IN = \"../data/raw/\"\n",
    "    directory = DATA_IN + \"rader/sm_cny_data_1_21_21.csv\"\n",
    "    us = pd.read_csv(directory)\n",
    "\n",
    "    masks = [\n",
    "        \"likely_wear_mask_exercising_outside\",\n",
    "        \"likely_wear_mask_grocery_shopping\",\n",
    "        \"likely_wear_mask_visit_family_friends\",\n",
    "        \"likely_wear_mask_workplace\",\n",
    "    ]\n",
    "    # weights = [\"weight_daily_national_13plus\", \"weight_state_weekly\"]\n",
    "    us = us[[\"response_date\", \"state\"] + masks]  # + weights\n",
    "\n",
    "    codes = pd.read_excel(DATA_IN + \"rader/cny_sm_codebook_2_5_21.xls\")\n",
    "    num2name = codes[codes[\"column\"] == \"state\"][[\"value\", \"label\"]]\n",
    "    us = pd.merge(us, num2name, left_on=\"state\", right_on=\"value\").drop(\n",
    "        [\"value\", \"state\"], axis=1\n",
    "    )\n",
    "    us[\"response_date\"] = pd.to_datetime(us[\"response_date\"])\n",
    "    us[\"percent_mc\"] = mean_shop_work(us, THRESHOLD)\n",
    "\n",
    "    us = (\n",
    "        us[[\"response_date\", \"label\", \"percent_mc\"]]\n",
    "        .groupby([\"response_date\", \"label\"])\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    if SMOOTH_RADER:\n",
    "        us = smooth_rader(us)\n",
    "\n",
    "    return us\n",
    "\n",
    "\n",
    "def mean_shop_work(df, THRESHOLD=2):\n",
    "    venues = [\"likely_wear_mask_grocery_shopping\", \"likely_wear_mask_workplace\"]\n",
    "    df[\"percent_mc\"] = df[venues].mean(axis=1)\n",
    "\n",
    "    return df[\"percent_mc\"] <= THRESHOLD\n",
    "\n",
    "\n",
    "def smooth_rader(df, win=7):\n",
    "    for r in df.label.unique():\n",
    "        s = df[df.label == r]\n",
    "        s[\"percent_mc\"] = smooth(s[\"percent_mc\"], window_len=win)[: -win + 1]\n",
    "        df[df.label == r] = s\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def smooth(x, window_len=7):\n",
    "    l = window_len\n",
    "    s = np.r_[x[l - 1 : 0 : -1], x, x[-2 : -l - 1 : -1]]\n",
    "    w = np.ones(window_len, \"d\")\n",
    "\n",
    "    return np.convolve(w / w.sum(), s, mode=\"valid\")\n",
    "\n",
    "def join_ox_umd(oxcgrt, wearing, npi_cols):\n",
    "    join = oxcgrt.merge(\n",
    "        wearing,\n",
    "        right_on=[\"country\", \"date\"],\n",
    "        left_on=[\"CountryName\", \"date\"],\n",
    "        suffixes=(\"\", \"_\"),\n",
    "    )  # , \\\n",
    "    # how='left')\n",
    "\n",
    "    return join[npi_cols + [\"country\", \"date\", \"ConfirmedCases\", \"ConfirmedDeaths\"]]\n",
    "\n",
    "\n",
    "def add_dummy_wearing_us(us, backfill=True):\n",
    "    rader_start = us.date.iloc[0] - timedelta(days=1)\n",
    "    fill_days = pd.date_range(Ds[0], rader_start, freq=\"D\")\n",
    "\n",
    "    Rs = us.country.unique()\n",
    "\n",
    "    if backfill:\n",
    "        for s in Rs:\n",
    "            df = pd.DataFrame(columns=[\"date\", \"country\", \"percent_mc\"])\n",
    "            df.date = fill_days\n",
    "            df.country = s\n",
    "            fill = us.set_index([\"country\", \"date\"]).loc[s].percent_mc.iloc[0]\n",
    "            df.percent_mc = fill\n",
    "            us = pd.concat([df, us])\n",
    "    # totally random dummy\n",
    "    else:\n",
    "        for s in us.country.unique():\n",
    "            df = pd.DataFrame(columns=[\"date\", \"country\", \"percent_mc\"])\n",
    "            df.date = fill_days\n",
    "            df.country = s\n",
    "            df.percent_mc = np.random.random(len(df))\n",
    "            us = pd.concat([df, us])\n",
    "\n",
    "    us = us.sort_values([\"date\", \"country\"])\n",
    "    return us\n",
    "\n",
    "\n",
    "def fill_missing_days(df):\n",
    "    df = df.set_index([\"date\", \"country\"])\n",
    "    df = df.unstack(fill_value=-1).asfreq(\"D\", fill_value=-1).stack().reset_index()\n",
    "    df = df.replace(-1, np.nan)\n",
    "\n",
    "    return interpolate_wearing_fwd_bwd(df)\n",
    "\n",
    "\n",
    "def interpolate_wearing_fwd_bwd(df):\n",
    "    regions = df.country.unique()\n",
    "    cs = []\n",
    "\n",
    "    for r in regions:\n",
    "        c = df[df.country == r]\n",
    "        c = c.set_index(\"date\")\n",
    "        c = c.interpolate(method=\"time\", limit_direction=\"both\").reset_index()\n",
    "        cs.append(c)\n",
    "\n",
    "    return pd.concat(cs)\n",
    "\n",
    "\n",
    "npi_cols_raw_no_mob = [\n",
    "            \"C1_School closing\",\n",
    "            \"C2_Workplace closing\",\n",
    "            \"C4_Restrictions on gatherings\",\n",
    "            \"C6_Stay at home requirements\",\n",
    "            \"C7_Restrictions on internal movement\",\n",
    "        ] \\\n",
    "    + [\"H6_Facial Coverings\"] \\\n",
    "    + [\"percent_mc\"]\n",
    "\n",
    "\n",
    "oxcgrt2 = oxcgrt.copy()\n",
    "col = \"H6_Facial Coverings\"\n",
    "# oxcgrt2[\"H6_Facial Coverings\"] = (oxcgrt2[col] < 2) & (oxcgrt2[col] == 1) & (oxcgrt2[f\"H6_Flag\"] == 1) #\n",
    "oxcgrt2[\"H6_Facial Coverings\"] = threshold(oxcgrt2, \"H6_Facial Coverings\", 1)\n",
    "wearing = load_and_clean_wearing()\n",
    "join = join_ox_umd(oxcgrt2, wearing, npi_cols_raw_no_mob)\n",
    "join = join.set_index([\"country\", \"date\"])\n",
    "#[[\"CountryName\", \"H6_Facial Coverings\", \"ConfirmedCases\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join[[\"H6_Facial Coverings\",\"percent_mc\"]].corr()\n",
    "import scipy\n",
    "round(scipy.stats.pearsonr(join[\"H6_Facial Coverings\"], join[\"percent_mc\"])[0], 2), \\\n",
    "round(scipy.stats.spearmanr(join[\"H6_Facial Coverings\"], join[\"percent_mc\"])[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mobility_data\n",
    "summaries = mw.get_centred_summary([\"Netherlands\"], df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "c = \"Netherlands\"\n",
    "cs = summaries[summaries.country == c]\n",
    "ax.plot(cs.day, cs.percent_mc * 100)# label=\"median wearing\")\n",
    "\n",
    "if len(c) > 20:\n",
    "    tsize = 10\n",
    "ax.set_title(c, fontsize=10)\n",
    "\n",
    "\n",
    "ax.axvline(x=0, color=\"black\", linestyle=\"--\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xlim(-20, 20)\n",
    "\n",
    "fig.text(0.5, -0.02, 'Days since mandate', ha='center', fontsize=16)\n",
    "fig.text(-0.02, 0.5, \"% mask wearing\", va='center', fontsize=16, rotation='vertical')\n",
    "plt.tight_layout(pad=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
